{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "jcr_year = '2021'\n",
    "datapath = f'../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar nodos a diccionario\n",
    "nodos = {}\n",
    "i=0\n",
    "nodosF = pd.read_excel(f\"{datapath}/0nodes_v5.0.xlsx\")\n",
    "for indice, fila in nodosF.iterrows():\n",
    "    id =fila[\"Id\"]\n",
    "    nodos[i] = id\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importar matriz de adyacencia (MA) de fichero. csv de enteros separados por ;\n",
    "MA = np.genfromtxt(f\"{datapath}/0edges_v5.0.csv\", delimiter=';', dtype='int', filling_values=0)\n",
    "print('dimensión Matriz Adj:', MA.shape)\n",
    "#print(nodos)\n",
    "#print(MA)\n",
    "#print(MA.T) #transpuesta de la matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construir grafo a partir de matriz\n",
    "r,s = MA.shape\n",
    "sMA = sparse.csr_matrix(MA)\n",
    "#print(sMA)\n",
    "G2 = nx.algorithms.bipartite.matrix.from_biadjacency_matrix(sMA)\n",
    "#cambiar nombre nodos\n",
    "G2 = nx.relabel_nodes(G2, nodos)\n",
    "#print(G2.nodes)\n",
    "#print(nx.is_connected(G2))\n",
    "#print(nx.number_connected_components(G2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1nodes = set(n for n,d in G2.nodes(data=True) if d['bipartite']==0)\n",
    "set2nodes = set(G2) - set1nodes\n",
    "print(\"Is connected: \"+str(nx.is_connected(G2)))\n",
    "print(\"# connected components: \"+str(nx.number_connected_components(G2)))\n",
    "print(\"Graph density: \"+str(nx.density(G2)))\n",
    "print(\"Bipartite nodes: \"+str(len(G2.nodes)))\n",
    "print(\"Bipartite set 1 nodes: \"+str(len(set1nodes)))\n",
    "print(\"Bipartite set 2 nodes: \"+str(len(set2nodes)))      \n",
    "print(\"Bipartite edges: \"+str(len(G2.edges)))\n",
    "print(\"Bipartite density: \"+str(nx.bipartite.density(G2, set1nodes)))\n",
    "#Generar el subgrafo con solo la componente principal\n",
    "#COMENTAR ESTAS LINEAS PARA TRABAJAR CON GRAFO COMPLETO\n",
    "#https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html#networkx.algorithms.components.connected_components\n",
    "#largest_cc = max(nx.connected_components(G2), key=len)\n",
    "#SG2 = G2.subgraph(largest_cc).copy()\n",
    "#print(nx.is_connected(SG2))\n",
    "#print(nx.number_connected_components(SG2))\n",
    "#G2 = SG2 #machacar el oríginal para hacer el resto de cálculos que siguen con G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargar atributos de los nodos de fichero. \"Id\" se utiliza como índice\n",
    "personasDf = pd.read_excel(f\"{datapath}/1EBMembers_v5.0_Final.xlsx\", index_col=0)\n",
    "print(personasDf.shape)\n",
    "print(personasDf.dtypes)\n",
    "#print(personasDf.head())\n",
    "#print(personasDf.tail())\n",
    "for indice, fila in personasDf.iterrows():\n",
    "    if(indice in G2.nodes):\n",
    "        G2.nodes[indice][\"Name\"]=fila[\"Name\"]\n",
    "        G2.nodes[indice][\"AffiliationName\"]=fila[\"AffiliationName\"]\n",
    "        # G2.nodes[indice][\"GenderMC\"]=fila[\"GenderMC\"]\n",
    "        # G2.nodes[indice][\"GenderAPI\"]=fila[\"GenderAPI\"]\n",
    "        G2.nodes[indice][\"Country\"]=fila[\"AffiliationCountry\"]\n",
    "        # G2.nodes[indice][\"GeoAreaMC\"]=fila[\"GeoAreaMC\"]\n",
    "    \n",
    "#cargar los nodos de los atributos (revistas) de fichero\n",
    "revistasDf = pd.read_excel(f\"{datapath}/2Journals_v5.0_crosslisted.xlsx\", index_col=0)\n",
    "print(revistasDf.shape)\n",
    "print(revistasDf.dtypes)\n",
    "#print(revistasDf.head())\n",
    "for indiceR, filaR in revistasDf.iterrows():\n",
    "    if(indiceR in G2.nodes):\n",
    "        G2.nodes[indiceR][\"JName\"]=filaR[\"JName\"]\n",
    "        G2.nodes[indiceR][\"Field\"]=filaR[\"Field\"]\n",
    "        G2.nodes[indiceR][\"Quartile\"]=filaR[\"Quartile\"]\n",
    "        # G2.nodes[indiceR][\"NumMembersEB\"]=filaR[\"NumMembersEB\"]\n",
    "        G2.nodes[indiceR][\"IF\"]=filaR[\"IF\"]\n",
    "        G2.nodes[indiceR][\"Cross-listed\"]=filaR[\"Cross-listed\"]\n",
    "        G2.nodes[indiceR][\"Cross-fields\"]=filaR[\"Cross-fields\"]\n",
    "        G2.nodes[indiceR][\"Groups\"]=filaR[\"Groups\"]\n",
    "        G2.nodes[indiceR][\"Cross-Groups\"]=filaR[\"Cross-groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ESTO SON PRUEBAS de carga. QUITAR\n",
    "#Asegurarse que a bipartite=0 van los members y a bipartite=1 van los journals.\n",
    "\n",
    "#PILLA Gender y GeoArea COMO FLOAT. Y también tienen missing values\n",
    "print(G2.nodes[23])\n",
    "print(G2.nodes[4504])\n",
    "print(G2.nodes[15084])\n",
    "print(G2.nodes['J00002'])\n",
    "print(G2.nodes['J00051'])\n",
    "print(G2.nodes['J00281'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener nodos de las dos particiones (RB_top y RB_Bottom)\n",
    "RB_top = {n for n, d in G2.nodes(data=True) if d['bipartite']==0}\n",
    "RB_bottom = set(G2) - RB_top\n",
    "#print(RB_top)\n",
    "#print(RB_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar grafo\n",
    "nx.write_gexf(G2, f\"{datapath}/complete_bipartite_graph_Allfields.gexf\")\n",
    "#nx.write_graphml(G2, \"data/grafo_revistas.graphml\") #no va. da error\n",
    "#nx.write_gml(G2, \"data/grafo_revistas.gml\") #no chufla. da error. guarda solamente los nodos\n",
    "#nx.write_pajek(G2, \"data/grafo_revistas.net\") #tampoco va.\n",
    "#probé con todos los formatos que soporta gephi 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comprobaciones de particiones y nodos\n",
    "#print(\"---------Partición1: Personas---------\")\n",
    "#print(RB_top)\n",
    "#print(\"\\n---------Partición2: Revistas---------\")\n",
    "#print(RB_bottom)\n",
    "#print(\"\\n---------Nodos---------\")\n",
    "#print(nodos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#métricas del grafo bipartito\n",
    "print(\"Es bipartito:\", bipartite.is_bipartite(G2))\n",
    "print(\"Densidad top:\",bipartite.density(G2, RB_top))\n",
    "#print(\"Densidad bottom:\",bipartite.density(G2, RB_bottom))\n",
    "#print(\"Avg. Clustering:\",bipartite.average_clustering(G2))\n",
    "\n",
    "#tengo dudas de que sea válido para un grafo bipartito\n",
    "#print(\"Avg. Shortest path:\",nx.average_shortest_path_length(G2)) #tarda un poco. Da error pq la revista WrittenCommunication no comparte ningún miembro con ninguna otra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##degree_centrality\n",
    "dc_top = bipartite.degree_centrality(G2, RB_top)\n",
    "dc_bottom = bipartite.degree_centrality(G2, RB_bottom)\n",
    "## aparentemente los dos generan los mismos datos. confirmado al exportar el dataset. se quita en todo lo que sigue\n",
    "##print(\"top\")\n",
    "##print(dc_top)\n",
    "##print(\"bottom\")\n",
    "##print(dc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DE MOMENTO COMENTAR O RELLENAR CON 0s. Al final habrá que calcularlo pero para las pruebas mejor sacarlo.\n",
    "\n",
    "##closeness_centrality. tarda un poco\n",
    "#cc_top = bipartite.closeness_centrality(G2, RB_top)\n",
    "##cc_bottom = bipartite.closeness_centrality(G2, RB_bottom) # aparentemente los dos generan los mismos datos\n",
    "##print(\"top\")\n",
    "##print(cc_top)\n",
    "##print(\"bottom\")\n",
    "##print(cc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DE MOMENTO COMENTAR O RELLENAR CON 0s. Al final habrá que calcularlo pero para las pruebas mejor sacarlo.\n",
    "\n",
    "##betweenness_centrality. tarda un poco\n",
    "#bc_top = bipartite.betweenness_centrality(G2, RB_top)\n",
    "##bc_bottom = bipartite.betweenness_centrality(G2, RB_bottom) # aparentemente los dos generan los mismos datos\n",
    "##print(\"top\")\n",
    "##print(bc_top)\n",
    "##print(\"bottom\")\n",
    "##print(bc_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DE MOMENTO COMENTAR O RELLENAR CON 0s. Al final habrá que calcularlo pero para las pruebas mejor sacarlo.\n",
    "\n",
    "#clustering (según la documentacion puede ser para todo el grafo)\n",
    "clustering_coeff = bipartite.clustering(G2)\n",
    "#print(clustering_coeff)\n",
    "\n",
    "##networkx también permite calcular Redundancy.\n",
    "##red = bipartite.node_redundancy(G2, RB_bottom) #pdte de procesar. no va para una red no conectada\n",
    "##print(red)\n",
    "##para RB_top (personas) da un error (solo se puede calcular para nodos que tengan vecinos comunes), por lo que no tiene mucho sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##append de las métricas a los dataframes iniciales (personasDf, revistasDf)\n",
    "##append degree centrality (dc_top)\n",
    "dcTop_Df = pd.DataFrame.from_dict(dc_top, orient='index', columns=['degreeCentrality'])\n",
    "resultPersonasDf = personasDf.join(dcTop_Df) #importante usar un DF distinto la primera ver por si se quiere ejecutar la celda más de una vez. si no da error al intentar duplicar columnas\n",
    "resultRevistasDf = revistasDf.join(dcTop_Df) #importante usar un DF distinto también\n",
    "dcBottom_Df = pd.DataFrame.from_dict(dc_bottom, orient='index', columns=['degreeCentralityBottom'])\n",
    "resultPersonasDf = resultPersonasDf.join(dcBottom_Df)\n",
    "resultRevistasDf = resultRevistasDf.join(dcBottom_Df)\n",
    "\n",
    "##append closeness centrality (cc_top)\n",
    "#ccTop_Df = pd.DataFrame.from_dict(cc_top, orient='index', columns=['closenessCentrality'])\n",
    "#resultPersonasDf = resultPersonasDf.join(ccTop_Df)\n",
    "#resultRevistasDf = resultRevistasDf.join(ccTop_Df)\n",
    "##ccBottom_Df = pd.DataFrame.from_dict(cc_bottom, orient='index', columns=['closenessCentralityBottom'])\n",
    "##resultPersonasDf = resultPersonasDf.join(ccBottom_Df)\n",
    "##resultRevistasDf = resultRevistasDf.join(ccBottom_Df)\n",
    "\n",
    "##append betweenness centrality (bc_top)\n",
    "#bcTop_Df = pd.DataFrame.from_dict(bc_top, orient='index', columns=['betweennessCentrality'])\n",
    "#resultPersonasDf = resultPersonasDf.join(bcTop_Df)\n",
    "#resultRevistasDf = resultRevistasDf.join(bcTop_Df)\n",
    "##bcBottom_Df = pd.DataFrame.from_dict(bc_bottom, orient='index', columns=['betweennessCentralityBottom'])\n",
    "##resultPersonasDf = resultPersonasDf.join(bcBottom_Df)\n",
    "##resultRevistasDf = resultRevistasDf.join(bcBottom_Df)\n",
    "\n",
    "#append clustering coefficient (clustering_coeff)\n",
    "clustering_coeff_Df = pd.DataFrame.from_dict(clustering_coeff, orient='index', columns=['clusteringCoefficient'])\n",
    "resultPersonasDf = resultPersonasDf.join(clustering_coeff_Df)\n",
    "resultRevistasDf = resultRevistasDf.join(clustering_coeff_Df)\n",
    "\n",
    "##append redundancy a revistas\n",
    "##red_Df = pd.DataFrame.from_dict(red, orient='index', columns=['Redundancy'])\n",
    "##resultRevistasDf = resultRevistasDf.join(red_Df)\n",
    "\n",
    "##guardar Excels con los datasets completos\n",
    "resultPersonasDf.to_excel(f\"{datapath}/RMetricsMembersDS.xlsx\")\n",
    "resultRevistasDf.to_excel(f\"{datapath}/RMetricsJournalsDS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTO HACE FALTA. El parámetro d de la parte final se ajusta después.\n",
    "\n",
    "#obtener proyecciones de revistas y personas.\n",
    "# #se podría usar weighted_projected_graph, que asigna peso a las aristas en función del número de vecinos comunes\n",
    "# GPersonas = bipartite.projected_graph(G2, RB_top)\n",
    "# nx.write_gexf(GPersonas, f\"{datapath}/grafo_proy_personas.gexf\")\n",
    "\n",
    "# GRevistas = bipartite.projected_graph(G2, RB_bottom)\n",
    "# nx.write_gexf(GRevistas, f\"{datapath}/grafo_proy_revistas.gexf\")\n",
    "\n",
    "# print(\"Densidades iniciales de las proyecciones\")\n",
    "# print(nx.density(GPersonas))\n",
    "# print(nx.density(GRevistas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Sacar las proyecciones que cumplan determinados criterios.\n",
    "# #primero sacar los weighted\n",
    "# GPersonasW = bipartite.weighted_projected_graph(G2, RB_top)\n",
    "# nx.write_gexf(GPersonasW, f\"{datapath}/grafo_proy_personasWeightedCompleto.gexf\")\n",
    "\n",
    "# GRevistasW = bipartite.weighted_projected_graph(G2, RB_bottom)\n",
    "# nx.write_gexf(GRevistasW, f\"{datapath}/grafo_proy_revistasWeightedCompleto.gexf\")\n",
    "\n",
    "# print(\"Densidades weighted graphs (deberían ser las mismas)\")\n",
    "# print(nx.density(GPersonasW))\n",
    "# print(nx.density(GRevistasW))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #quitar los edges con weight<x para personas \n",
    "# listaEdgesPersonasBorrar = []\n",
    "# for (u,v,d) in GPersonasW.edges(data='weight'):\n",
    "#     if d<3: listaEdgesPersonasBorrar.append((u,v))\n",
    "# GPersonasW.remove_edges_from(listaEdgesPersonasBorrar)\n",
    "# #y con weight<x para revistas\n",
    "# listaEdgesRevistasBorrar = []\n",
    "# for (u,v,d) in GRevistasW.edges(data='weight'):\n",
    "#     if d<7: listaEdgesRevistasBorrar.append((u,v))\n",
    "# GRevistasW.remove_edges_from(listaEdgesRevistasBorrar)\n",
    "# #guardar grafos\n",
    "# nx.write_gexf(GPersonasW, f\"{datapath}/grafo_proy_personasWeightedMayorIgual3.gexf\")\n",
    "# nx.write_gexf(GRevistasW, f\"{datapath}/grafo_proy_revistasWeightedMayorIgual7.gexf\")\n",
    "# print(\"Densidades tras eliminar edges que cumplen los criterios\")\n",
    "# print(nx.density(GPersonasW))\n",
    "# print(nx.density(GRevistasW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUERA\n",
    "\n",
    "#Create a boxplots\n",
    "# resultPersonasDf.boxplot('degreeCentrality', by='GenderAPI', figsize=(12,8))\n",
    "# resultPersonasDf.boxplot('closenessCentrality', by='GenderAPI', figsize=(12,8)) #Puede no ser representativo. Grafo no conexo\n",
    "# resultPersonasDf.boxplot('betweennessCentrality', by='GenderAPI', figsize=(12,8))\n",
    "# resultPersonasDf.boxplot('clusteringCoefficient', by='GenderAPI', figsize=(12,8))\n",
    "\n",
    "#He quitado todas las estadísticas que heredé del anterior. Creo que es mejor hacerlas en MiniTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear grafo revistas-afilicaiones (G3). \n",
    "G3=nx.Graph()\n",
    "#inicializar grafo solo con revistas (nodos)\n",
    "for n in G2.nodes:\n",
    "    if G2.nodes[n][\"bipartite\"]: G3.add_node(n, bipartite=0, Name=G2.nodes[n][\"JName\"])\n",
    "\n",
    "#print(G3.nodes['J001'])\n",
    "#print(G3.nodes['J129'])\n",
    "#print(G3.nodes['J130'])\n",
    "#print(G3.nodes['J281'])\n",
    "\n",
    "#añadir las afiliaciones (nodos)\n",
    "for n in G2.nodes:\n",
    "    if G2.nodes[n][\"bipartite\"]==0:\n",
    "        #print(G2.nodes[n][\"Affiliation\"])\n",
    "        if G2.nodes[n][\"AffiliationName\"] not in G3:\n",
    "            #Affiliation no está en grafo. Añadir\n",
    "            G3.add_node(G2.nodes[n][\"AffiliationName\"], bipartite=1, Name=G2.nodes[n][\"AffiliationName\"], Scholars=1)\n",
    "        else:\n",
    "            G3.nodes[G2.nodes[n][\"AffiliationName\"]][\"Scholars\"] +=1\n",
    "\n",
    "# print(G3.nodes['University of California Santa Barbara'])\n",
    "# print(G3.nodes['University of Padova'])\n",
    "# print(G3.nodes['University of Pennsylvania'])            \n",
    "\n",
    "#añadir aristas (con peso)\n",
    "for n in G2.nodes:\n",
    "    if G2.nodes[n][\"bipartite\"]==0: #por cada autor\n",
    "        #print(\"nodo:\", n, type(n))\n",
    "        for e in G2[n]: #por cada arista (revistas de las que es miembro)\n",
    "            affil = G2.nodes[n][\"AffiliationName\"]\n",
    "            arista = (e, affil)\n",
    "            #print(\"arista:\",e, affil,arista in G3.edges)\n",
    "            if arista in G3.edges:\n",
    "                #ya hay otro miembro anterior de esa revista-institución\n",
    "                G3.edges[e, affil][\"weight\"] +=1\n",
    "            else:\n",
    "                #no hay ninguno anterior\n",
    "                G3.add_edge(e, affil, weight=1)              \n",
    "\n",
    "#print(G3.edges)\n",
    "            \n",
    "#métricas del grafo bipartito\n",
    "print(\"Es bipartito:\", bipartite.is_bipartite(G3))\n",
    "RB_topG3 = {n for n, d in G3.nodes(data=True) if d[\"bipartite\"]==0}\n",
    "RB_bottomG3 = set(G3) - RB_topG3\n",
    "print(\"Densidad top:\",bipartite.density(G3, RB_topG3))\n",
    "print(\"Densidad bottom:\",bipartite.density(G3, RB_bottomG3))\n",
    "print(\"Avg. Clustering:\",bipartite.average_clustering(G3))\n",
    "\n",
    "#guardar grafo\n",
    "nx.write_gexf(G3, f\"{datapath}/grafo_bipartito_revistas_afiliaciones.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOS CÁLCULOS DE LAS TRES ÚLTIMAS MÉTRICAS SE PUEDEN COMENTAR SI TARDAN DEMASIADO\n",
    "\n",
    "#métricas de red instituciones-revistas\n",
    "#degree_centrality\n",
    "dc_topG3 = bipartite.degree_centrality(G3, RB_topG3)\n",
    "\n",
    "#closeness_centrality. tarda un poco\n",
    "cc_topG3 = bipartite.closeness_centrality(G3, RB_topG3)\n",
    "\n",
    "# LAS DOS SIGUIENTES TARDAN BASTANTE, NO HASTA EL PUNTO DE SER INCALCULABLES, PERO LAS COMENTO PARA ACELERAR \n",
    "#betweenness_centrality. tarda un poco\n",
    "#bc_topG3 = bipartite.betweenness_centrality(G3, RB_topG3)\n",
    "\n",
    "#clustering (según la documentacion puede ser para todo el grafo)\n",
    "#clustering_coeffG3 = bipartite.clustering(G3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar dataset instituciones-revistas\n",
    "#primero crear DF solo afiliaciones. Están en RB_bottomG3 (type=set)\n",
    "institucionesDf = pd.DataFrame(index=list(RB_bottomG3))\n",
    "\n",
    "#append degree centrality (dc_topG3)\n",
    "dcTopG3_Df = pd.DataFrame.from_dict(dc_topG3, orient='index', columns=['degreeCentrality'])\n",
    "resultInstitucionesDf = institucionesDf.join(dcTopG3_Df) #importante usar un DF distinto la primera ver por si se quiere ejecutar la celda más de una vez. si no da error al intentar duplicar columnas\n",
    "\n",
    "#append closeness centrality (cc_topG3)\n",
    "ccTopG3_Df = pd.DataFrame.from_dict(cc_topG3, orient='index', columns=['closenessCentrality'])\n",
    "resultInstitucionesDf = resultInstitucionesDf.join(ccTopG3_Df)\n",
    "\n",
    "#append betweenness centrality (bc_topG3)\n",
    "#bcTopG3_Df = pd.DataFrame.from_dict(bc_topG3, orient='index', columns=['betweennessCentrality'])\n",
    "#resultInstitucionesDf = resultInstitucionesDf.join(bcTopG3_Df)\n",
    "\n",
    "#append clustering coefficient (clustering_coeffG3)\n",
    "#clustering_coeffG3_Df = pd.DataFrame.from_dict(clustering_coeffG3, orient='index', columns=['clusteringCoefficient'])\n",
    "#resultInstitucionesDf = resultInstitucionesDf.join(clustering_coeffG3_Df)\n",
    "\n",
    "#append num de scholars de cada institución\n",
    "#primero sacarlo del grafo y convertirlo en diccionario\n",
    "num_scholars_dict = dict()\n",
    "for n in G3.nodes:\n",
    "    if G3.nodes[n][\"bipartite\"]==1: #por cada institución\n",
    "        num_scholars_dict[G3.nodes[n][\"Name\"]] = G3.nodes[n][\"Scholars\"]\n",
    "#añadirlo al dataset\n",
    "num_scholarsG3_Df = pd.DataFrame.from_dict(num_scholars_dict, orient='index', columns=['numScholars'])\n",
    "resultInstitucionesDf = resultInstitucionesDf.join(num_scholarsG3_Df)\n",
    "\n",
    "#guardar Excel con dataset completo\n",
    "resultInstitucionesDf.to_excel(f\"{datapath}/RinstitucionesRevistasDS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Sacar las proyecciones que cumplan determinados criterios.\n",
    "#primero sacar los weighted\n",
    "\n",
    "\n",
    "GInstitucionesW = bipartite.weighted_projected_graph(G3, RB_bottomG3)\n",
    "nx.write_gexf(GInstitucionesW, f\"{datapath}/grafo_proy_institucionesWeightedCompleto.gexf\")\n",
    "print(\"Densidad weighted graph (debería ser la misma):\", nx.density(GInstitucionesW))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #quitar los edges con weight<x \n",
    "listaEdgesInstitucionesBorrar = []\n",
    "for (u,v,d) in GInstitucionesW.edges(data='weight'):\n",
    "    if d<20: listaEdgesInstitucionesBorrar.append((u,v))\n",
    "GInstitucionesW.remove_edges_from(listaEdgesInstitucionesBorrar)\n",
    "#guardar grafo\n",
    "nx.write_gexf(GInstitucionesW, f\"{datapath}/grafo_proy_InstitucionesWeightedMayorIgual20.gexf\")\n",
    "print(\"Densidad tras eliminar edges que cumplen criterio:\", nx.density(GInstitucionesW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear grafo solo con los miembros de instituciones que tengan X o más en todo el dataset\n",
    "\n",
    "GParcial = nx.Graph(G2) #uso una copia para no modificar el original\n",
    "\n",
    "for n in G2.nodes:\n",
    "    if G2.nodes[n][\"bipartite\"]==0: #por cada miembro\n",
    "        affil = G2.nodes[n][\"AffiliationName\"]\n",
    "        if G3.nodes[affil][\"Scholars\"] < 50: #si tiene menos de X members en el grafo inicial\n",
    "            GParcial.remove_node(n)\n",
    "            \n",
    "#guardar grafo\n",
    "nx.write_gexf(GParcial, f\"{datapath}/grafo_parcialInstituciones.gexf\")\n",
    "print(\"Densidad grafo parcial:\", nx.density(GParcial))\n",
    "\n",
    "RB_topGParcial = {n for n, d in GParcial.nodes(data=True) if d[\"bipartite\"]==0}\n",
    "RB_bottomGParcial = set(GParcial) - RB_topGParcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#métricas de red grafo parcial\n",
    "#degree_centrality\n",
    "dc_topGParcial = bipartite.degree_centrality(GParcial, RB_topGParcial)\n",
    "#dc_topGParcial\n",
    "\n",
    "# ESTA TARDA MUCHO, LA COMENTO\n",
    "#closeness_centrality. tarda un poco\n",
    "#cc_topGParcial = bipartite.closeness_centrality(GParcial, RB_topGParcial)\n",
    "#cc_topGParcial\n",
    "\n",
    "# ESTA TARDA MUCHO, LA COMENTO\n",
    "#betweenness_centrality. tarda un poco\n",
    "#bc_topGParcial = bipartite.betweenness_centrality(GParcial, RB_topGParcial)\n",
    "#bc_topGParcial\n",
    "\n",
    "# ESTA LA COMENTO TAMBIÉN EN PREVISIÓN DE QUE TARDARÁ MUCHO\n",
    "#clustering (según la documentacion puede ser para todo el grafo)\n",
    "#clustering_coeffGParcial = bipartite.clustering(GParcial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardar dataset grafo parcial\n",
    "#primero crear DF solo miembros. Están en RB_topGParcial (type=set)\n",
    "dictNames = dict(GParcial.nodes.data('Name'))\n",
    "parcialDf = pd.DataFrame.from_dict(dictNames, orient='index', columns=['Name'])\n",
    "print(parcialDf.head())\n",
    "#añadir resto de columnas\n",
    "dictAffiliations = dict(GParcial.nodes.data('AffiliationName'))\n",
    "affiliations_Df = pd.DataFrame.from_dict(dictAffiliations, orient='index', columns=['AffiliationName'])\n",
    "resultParcialDf = parcialDf.join(affiliations_Df) #importante usar un DF distinto la primera ver por si se quiere ejecutar la celda más de una vez. si no da error al intentar duplicar columnas\n",
    "dictGender = dict(GParcial.nodes.data('Gender'))\n",
    "gender_Df = pd.DataFrame.from_dict(dictGender, orient='index', columns=['Gender'])\n",
    "resultParcialDf = resultParcialDf.join(gender_Df)\n",
    "dictCountry = dict(GParcial.nodes.data('Country'))\n",
    "country_Df = pd.DataFrame.from_dict(dictCountry, orient='index', columns=['Country'])\n",
    "resultParcialDf = resultParcialDf.join(country_Df)\n",
    "\n",
    "#append degree centrality (dc_topGParcial)\n",
    "dcTopGParcial_Df = pd.DataFrame.from_dict(dc_topGParcial, orient='index', columns=['degreeCentrality'])\n",
    "resultParcialDf = resultParcialDf.join(dcTopGParcial_Df) \n",
    "\n",
    "#append closeness centrality (cc_topGParcial)\n",
    "#ccTopGParcial_Df = pd.DataFrame.from_dict(cc_topGParcial, orient='index', columns=['closenessCentrality'])\n",
    "#resultParcialDf = resultParcialDf.join(ccTopGParcial_Df)\n",
    "\n",
    "#append betweenness centrality (bc_topGParcial)\n",
    "#bcTopGParcial_Df = pd.DataFrame.from_dict(bc_topGParcial, orient='index', columns=['betweennessCentrality'])\n",
    "#resultParcialDf = resultParcialDf.join(bcTopGParcial_Df)\n",
    "\n",
    "#append clustering coefficient (clustering_coeffGParcial)\n",
    "#clustering_coeffGParcial_Df = pd.DataFrame.from_dict(clustering_coeffGParcial, orient='index', columns=['clusteringCoefficient'])\n",
    "#resultParcialDf = resultParcialDf.join(clustering_coeffGParcial_Df)\n",
    "\n",
    "#guardar Excel con dataset completo\n",
    "resultParcialDf.to_excel(f\"{datapath}/RparcialInstitucionesDS.xlsx\")\n",
    "#HAY QUE QUITAR LAS REVISTAS DEL EXCEL FINAL. Lo más fácil es hacerlo a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTO NO SÉ SI LO VAMOS A USAR. DEPENDE DE LOS DATOS. HASTA EL FINAL NO LO VAMOS A SABER.\n",
    "#CREO QUE NO TARDA MUCHO\n",
    "\n",
    "# #obtener k-cores de las proyecciones de revistas y personas.\n",
    "# from networkx.algorithms import core\n",
    "# GRevistasKCore = core.k_core(GRevistas, k=5)\n",
    "# GPersonasKCore = core.k_core(GPersonas, k=421) #no sale nada. Desbalanceo entre disciplinas\n",
    "# nx.write_gexf(GRevistasKCore, f\"{datapath}/grafo_proy_revistasKCore.gexf\")\n",
    "# nx.write_gexf(GPersonasKCore, f\"{datapath}/grafo_proy_personasKCore.gexf\")\n",
    "# print(\"Densidad k-core weighted graphs\")\n",
    "# print(nx.density(GRevistasKCore))\n",
    "# print(nx.density(GPersonasKCore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear grafo revistas-fields (G4). \n",
    "G4=nx.Graph()\n",
    "#inicializar grafo solo con editores (nodos)\n",
    "for n in G2.nodes:\n",
    "    if G2.nodes[n][\"bipartite\"]==0: \n",
    "        G4.add_node(n, bipartite=1, Name=G2.nodes[n][\"Name\"], AffiliationName=G2.nodes[n][\"AffiliationName\"]\n",
    "                    ,Country=G2.nodes[n][\"Country\"])\n",
    "\n",
    "\n",
    "#añadir las fields (nodos)\n",
    "for n in G2.nodes:\n",
    "    if G2.nodes[n][\"bipartite\"]==1:\n",
    "        if G2.nodes[n][\"Field\"] not in G4:\n",
    "            #Field no está en grafo. Añadir\n",
    "            G4.add_node(G2.nodes[n][\"Field\"], bipartite=0, Name=G2.nodes[n][\"Field\"], Groups=G2.nodes[n][\"Groups\"], CrossGroups=G2.nodes[n][\"Cross-Groups\"], Journals=1)\n",
    "        else:\n",
    "            G4.nodes[G2.nodes[n][\"Field\"]][\"Journals\"] +=1\n",
    "\n",
    "#añadir aristas (con peso)\n",
    "for n in G2.nodes:\n",
    "    if G2.nodes[n][\"bipartite\"]==1: #por cada revista\n",
    "        #print(\"nodo:\", n, type(n))\n",
    "        for e in G2[n]: #por cada arista (miembros EB)\n",
    "            nField = G2.nodes[n][\"Field\"]\n",
    "            arista = (e, nField)\n",
    "            #print(\"arista:\",e, affil,arista in G3.edges)\n",
    "            if arista in G4.edges:\n",
    "                #ya hay otro miembro anterior de ese campo-autor\n",
    "                G4.edges[e, nField][\"weight\"] +=1\n",
    "            else:\n",
    "                #no hay ninguno anterior\n",
    "                G4.add_edge(e, nField, weight=1)    \n",
    "           \n",
    "\n",
    "#métricas del grafo bipartito\n",
    "print(\"Es bipartito:\", bipartite.is_bipartite(G4))\n",
    "RB_topG4 = {n for n, d in G4.nodes(data=True) if d[\"bipartite\"]==0}\n",
    "RB_bottomG4 = set(G4) - RB_topG4\n",
    "print(\"Densidad top:\",bipartite.density(G4, RB_topG4))\n",
    "print(\"Densidad bottom:\",bipartite.density(G4, RB_bottomG4))\n",
    "#print(\"Avg. Clustering:\",bipartite.average_clustering(G4)) #tarda bastante\n",
    "\n",
    "#guardar grafo\n",
    "nx.write_gexf(G4, f\"{datapath}/grafo_bipartito_EBs_fields.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE PUEDE QUITAR. Las métricas no las vamos a usar.\n",
    "\n",
    "# #métricas de red EBs-fields                  TARDA BASTANTE en total\n",
    "# #degree_centrality\n",
    "# dc_topG4 = bipartite.degree_centrality(G4, RB_topG4)\n",
    "# #print(dc_topG4)\n",
    "\n",
    "# #closeness_centrality. tarda un poco\n",
    "# cc_topG4 = bipartite.closeness_centrality(G4, RB_topG4)\n",
    "# #print(cc_topG4)\n",
    "\n",
    "# #betweenness_centrality. tarda un poco\n",
    "# bc_topG4 = bipartite.betweenness_centrality(G4, RB_topG4)\n",
    "# #print(bc_topG4)\n",
    "\n",
    "# #clustering (según la documentacion puede ser para todo el grafo)\n",
    "# clustering_coeffG4 = bipartite.clustering(G4)\n",
    "# #print(clustering_coeffG4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De esta celda no hace falta lo que guarda las métricas.\n",
    "#solo es útil lo que va al final de añadir el nº de journals de cada field\n",
    "\n",
    "\n",
    "#guardar dataset fields\n",
    "#primero crear DF solo fields. Están en RB_topG4 (type=set)\n",
    "resultFieldsDf = pd.DataFrame(index=list(RB_topG4))\n",
    "\n",
    "# #append degree centrality (dc_topG4)\n",
    "# dcTopG4_Df = pd.DataFrame.from_dict(dc_topG4, orient='index', columns=['degreeCentrality'])\n",
    "# resultFieldsDf = fieldsDf.join(dcTopG4_Df) #importante usar un DF distinto la primera ver por si se quiere ejecutar la celda más de una vez. si no da error al intentar duplicar columnas\n",
    "\n",
    "# #append closeness centrality (cc_topG4)\n",
    "# ccTopG4_Df = pd.DataFrame.from_dict(cc_topG4, orient='index', columns=['closenessCentrality'])\n",
    "# resultFieldsDf = resultFieldsDf.join(ccTopG4_Df)\n",
    "\n",
    "# #append betweenness centrality (bc_topG4)\n",
    "# bcTopG4_Df = pd.DataFrame.from_dict(bc_topG4, orient='index', columns=['betweennessCentrality'])\n",
    "# resultFieldsDf = resultFieldsDf.join(bcTopG4_Df)\n",
    "\n",
    "# #append clustering coefficient (clustering_coeffG4)\n",
    "# clustering_coeffG4_Df = pd.DataFrame.from_dict(clustering_coeffG4, orient='index', columns=['clusteringCoefficient'])\n",
    "# resultFieldsDf = resultFieldsDf.join(clustering_coeffG4_Df)\n",
    "\n",
    "#append num de journals de cada field\n",
    "#primero sacarlo del grafo y convertirlo en diccionario\n",
    "num_journals_dict = dict()\n",
    "groups_dict = dict()\n",
    "crossgroups_dict = dict()\n",
    "for n in G4.nodes:\n",
    "    if G4.nodes[n][\"bipartite\"]==0: #por cada field\n",
    "        node = G4.nodes[n]\n",
    "        num_journals_dict[node[\"Name\"]] = node[\"Journals\"]\n",
    "        groups_dict[node[\"Name\"]] = node[\"Groups\"]\n",
    "        crossgroups_dict[node[\"Name\"]] = node[\"CrossGroups\"]\n",
    "#añadirlo al dataset\n",
    "num_journalsG4_Df = pd.DataFrame.from_dict(num_journals_dict, orient='index', columns=['numJournals'])\n",
    "groups_df = pd.DataFrame.from_dict(groups_dict,orient='index', columns=['groups'])\n",
    "crossgroups_df = pd.DataFrame.from_dict(crossgroups_dict, orient='index', columns=['cross-groups'])\n",
    "resultFieldsDf = resultFieldsDf.join(num_journalsG4_Df).join(groups_df).join(crossgroups_df)\n",
    "\n",
    "#guardar Excel con dataset completo\n",
    "resultFieldsDf.to_excel(f\"{datapath}/RFieldsDS.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es importante. G4 es una proyección anterior\n",
    "\n",
    "#sacar proyección solo fields\n",
    "GFields = bipartite.projected_graph(G4, RB_topG4)\n",
    "nx.write_gexf(GFields, f\"{datapath}/grafo_proy_fields.gexf\")\n",
    "print(\"Densidad inicial de la proyeccion:\", nx.density(GFields))\n",
    "\n",
    "#Sacar las proyecciones que cumplan determinados criterios.\n",
    "#primero sacar los weighted\n",
    "GFieldsW = bipartite.weighted_projected_graph(G4, RB_topG4)\n",
    "nx.write_gexf(GFieldsW, f\"{datapath}/grafo_proy_fieldsWeightedCompleto.gexf\")\n",
    "print(\"Densidad weighted graph (debería ser la misma):\", nx.density(GFieldsW))\n",
    "\n",
    "#quitar los edges con weight<x \n",
    "listaEdgesFieldsBorrar = []\n",
    "for (u,v,d) in GFieldsW.edges(data='weight'):\n",
    "    if d<5: listaEdgesFieldsBorrar.append((u,v))\n",
    "GFieldsW.remove_edges_from(listaEdgesFieldsBorrar)\n",
    "#guardar grafo\n",
    "nx.write_gexf(GFieldsW, f\"{datapath}/grafo_proy_FieldsWeightedMayorIgual5.gexf\")\n",
    "print(\"Densidad tras eliminar edges que cumplen criterio:\", nx.density(GFieldsW))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "c116aac75534fcca1428568eb0dc7bfd9894aa23af5d90711a9b46889e33c11d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
